import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from torch.utils.data import TensorDataset, DataLoader, random_split

# Ð£ÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾ Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
model_name = "sberbank-ai/rugpt3large_based_on_gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(device)
model.eval()

# Ð”Ð°Ð½Ð½Ñ‹Ðµ
print("tokeizer")
print(tokenizer.tokenize("ÑÐ²Ð¾ÐµÐ¹"))
print(tokenizer.tokenize("Ñ‚ÐµÐ±Ñ"))
print(tokenizer.tokenize("ÐµÐ³Ð¾"))
print(tokenizer.tokenize("ÑÐ²Ð¾Ð¸Ð¼"))

true = ["Ð¾Ð½Ð° Ð´Ð¾Ñ€Ð¾Ð¶Ð¸Ñ‚", "Ñ Ð²ÑÑ‚Ñ€ÐµÑ‚Ð¸Ð»Ð°ÑÑŒ Ñ"]
true += [
    "Ñ Ð³Ð¾Ñ€Ð¶ÑƒÑÑŒ",
    "Ð¾Ð½ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚",
    "Ð¼Ñ‹ Ð·Ð°Ð½Ð¸Ð¼Ð°ÐµÐ¼ÑÑ",
    "Ð¾Ð½Ð° ÑƒÐ²Ð»ÐµÑ‡ÐµÐ½Ð°",
    "Ð¾Ð½Ð¸ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÑƒÑŽÑ‚ÑÑ",
    "Ñ Ð´Ð¾Ñ€Ð¾Ð¶Ñƒ",
    "Ñ‚Ñ‹ Ð²Ð¾ÑÑ…Ð¸Ñ‰Ð°ÐµÑˆÑŒÑÑ",
    "Ð¾Ð½ Ð²Ð»Ð°Ð´ÐµÐµÑ‚",
    "Ð¼Ñ‹ Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½Ñ‹",
    "Ð¾Ð½Ð¸ Ð²Ð¾ÑÑ…Ð¸Ñ‰Ð°ÑŽÑ‚ÑÑ",
    "Ð¾Ð½Ð° Ð½Ð°ÑÐ»Ð°Ð¶Ð´Ð°ÐµÑ‚ÑÑ",
    "Ñ‚Ñ‹ Ð¸Ð³Ñ€Ð°ÐµÑˆÑŒ Ñ",
    "Ñ ÑÐ¿Ð¾Ñ€ÑŽ Ñ",
    "Ð¼Ñ‹ Ð±Ð¾Ñ€ÐµÐ¼ÑÑ Ñ",
    "Ð¾Ð½ Ð´ÐµÐ»Ð¸Ñ‚ÑÑ Ñ",
    "Ñ‚Ñ‹ ÑÑ€Ð°Ð¶Ð°ÐµÑˆÑŒÑÑ Ñ",
    "Ð¾Ð½Ð° ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ",
    "Ð¾Ð½Ð¸ Ð·Ð½Ð°ÐºÐ¾Ð¼ÑÑ‚ÑÑ Ñ",
    "Ð¼Ñ‹ ÑˆÑƒÑ‚Ð¸Ð¼ Ð½Ð°Ð´",
    "Ð¾Ð½ ÑƒÑ…Ð°Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð·Ð°",
    "Ñ Ñ€ÑƒÐ³Ð°ÑŽÑÑŒ Ñ",
    "Ñ‚Ñ‹ Ð´Ñ€ÑƒÐ¶Ð¸ÑˆÑŒ Ñ",
    "Ð¾Ð½Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð½Ð°Ð´",
    "Ð¼Ñ‹ ÑÐ¾Ð²ÐµÑ‚ÑƒÐµÐ¼ÑÑ Ñ",
    "Ð¾Ð½Ð¸ ÑÐ¿Ð¾Ñ€ÑÑ‚ Ñ",
    "Ñ‚Ñ‹ Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÑ‹Ð²Ð°ÐµÑˆÑŒÑÑ Ñ",
    "Ð¾Ð½Ð° ÑÐ¼ÐµÑ‘Ñ‚ÑÑ Ð½Ð°Ð´",
    "Ñ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð°Ñ€Ð¸Ð²Ð°ÑŽ Ñ",
    "Ð¾Ð½ ÐºÐ¾Ð½ÐºÑƒÑ€Ð¸Ñ€ÑƒÐµÑ‚ Ñ",
    "Ð¼Ñ‹ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ñ"
]

true_case = ["Ð¿Ð°Ñ€Ð½ÐµÐ¼", "ÑÐ²Ð¾ÐµÐ¹", "ÑÐ²Ð¾Ð¸Ð¼", "Ð½Ð¸Ð¼", "Ñ‚ÐµÐ¼", "Ð¼ÑƒÐ¶Ñ‡Ð¸Ð½Ð¾Ð¹", "Ð´ÐµÐ²ÑƒÑˆÐºÐ¾Ð¹", "Ñ‡ÐµÐ»Ð¾Ð²ÐµÐºÐ¾Ð¼", "Ñ‚Ð¾Ð±Ð¾Ð¹", "Ð¸Ð¼", "ÑÑ‚Ð¸Ð¼", "Ð²Ð°Ð¼Ð¸"]
corrupted_acc = ["Ñ‚ÐµÐ±Ñ", "ÐµÐ³Ð¾", "Ñ‚Ð¾Ð³Ð¾", "Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°", "Ð¼ÐµÐ½Ñ", "Ð¼Ð°Ð»Ð¾Ð³Ð¾", "ÑÐµÐ±Ñ"]
corrupted = ["Ð¾Ð½Ð° Ð¾Ð±Ð¾Ð¶Ð°ÐµÑ‚", "Ñ Ð½Ð°Ñ‡Ð°Ð»Ð° Ñ"]
corrupted += [
    "Ñ Ð»ÑŽÐ±Ð»ÑŽ",
    "Ð¾Ð½ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð¸Ñ€ÑƒÐµÑ‚",
    "Ð¼Ñ‹ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÐµÐ¼",
    "Ð¾Ð½Ð° Ð²Ð¸Ð´Ð¸Ñ‚",
    "Ð¾Ð½Ð¸ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÑŽÑ‚",
    "Ñ Ñ…Ð¾Ñ‡Ñƒ",
    "Ñ‚Ñ‹ ÑÐ»ÑƒÑˆÐ°ÐµÑˆÑŒ",
    "Ð¾Ð½ Ð¸Ð·ÑƒÑ‡Ð°ÐµÑ‚",
    "Ð¼Ñ‹ Ð¾ÑÑƒÐ¶Ð´Ð°ÐµÐ¼",
    "Ð¾Ð½Ð¸ Ñ‚ÐµÑ€Ð¿ÑÑ‚",
    "Ð¾Ð½Ð° Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÑ‚",
    "Ñ‚Ñ‹ Ñ‡Ð¸Ñ‚Ð°ÐµÑˆÑŒ",
    "Ñ Ð·Ð½Ð°ÑŽ",
    "Ð¼Ñ‹ Ð¿Ð¾Ð¼Ð½Ð¸Ð¼",
    "Ð¾Ð½ Ð·Ð°Ð±Ñ‹Ð²Ð°ÐµÑ‚",
    "Ñ‚Ñ‹ Ð¶Ð´Ñ‘ÑˆÑŒ",
    "Ð¾Ð½Ð° Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚",
    "Ð¾Ð½Ð¸ Ð±Ñ€Ð¾ÑÐ°ÑŽÑ‚",
    "Ð¼Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼",
    "Ð¾Ð½ Ð·Ð°Ð¼ÐµÑ‡Ð°ÐµÑ‚",
    "Ñ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÑŽ",
    "Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð±ÑƒÐµÑˆÑŒ",
    "Ð¾Ð½Ð° ÑÑ‚Ñ€Ð¾Ð¸Ñ‚",
    "Ð¼Ñ‹ ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼",
    "Ð¾Ð½Ð¸ Ð¿Ð¾ÐºÑƒÐ¿Ð°ÑŽÑ‚",
    "Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑˆÑŒ",
    "Ð¾Ð½Ð° Ð¿Ð¸ÑˆÐµÑ‚",
    "Ñ Ð½Ð°Ñ…Ð¾Ð¶Ñƒ",
    "Ð¾Ð½ Ð»Ð¾Ð²Ð¸Ñ‚",
    "Ð¼Ñ‹ Ð²Ñ‹Ð½Ð¾ÑÐ¸Ð¼"
]

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ
clean_attn_output = []
corrupt_attn_output = []

attn_weights_storage = []

def get_attention_weights(prompt, storage):
    storage.clear()
    start_token_id = tokenizer.bos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    
    print("LEN INPUTS:", len(inputs))
    print("MI_INPUTS:,", inputs)
    with torch.no_grad():
        outputs = model(**inputs, output_attentions=True)

    attentions = outputs.attentions  
    last_layer_attn = attentions[-1].detach().cpu()  
    storage.append(last_layer_attn)



def print_topk_logits(hidden, k=5):
    logits = model.lm_head(hidden[0, -1])
    topk = torch.topk(logits, k)
    tokens = tokenizer.batch_decode(topk.indices)
    print("ðŸ” Top-5 predictions:")
    for token, score in zip(tokens, topk.values):
        print(f"   {token.strip():>10} ({score.item():.2f})")


def get_attention_output(prompt, storage):
    storage.clear()
    start_token_id = tokenizer.bos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

   
    def hook(module, input, output):
        # grabs attention output and stores it in storage 
        
        atten =  output[0].detach()
        print("shape of attention output!: ", atten.shape)
        storage.append(atten)

    # attachs hook to the last layer of attention (identified as important through logit lens)
    handle = model.transformer.h[-1].attn.register_forward_hook(hook)
    with torch.no_grad():
        # run foward pass. hook grabs the attention
        _ = model(**inputs)
    # remove hook
    handle.remove()
    return inputs




def get_logits_diff_by_category(hidden, positive_words, negative_words, top_k=10000):
    logits = model.lm_head(hidden[0, -1])
    topk = logits.topk(k=top_k, dim=-1) 
    tokens = tokenizer.batch_decode(topk.indices)

    top_pos = float('-inf')
    top_neg = float('-inf')
    best_pos = None
    best_neg = None

    print(positive_words)
    print(negative_words)
    for logit, token in zip(topk.values, tokens):
        stripped_token = token.strip()

        
        if stripped_token in positive_words and logit > top_pos:

            top_pos = logit.item()
            best_pos = stripped_token
        elif stripped_token in negative_words and logit > top_neg:
            top_neg = logit.item()
            best_neg = stripped_token

    if best_pos is None:
        print(f"âš ï¸ ÐÐ¸ Ð¾Ð´Ð½Ð¾ ÑÐ»Ð¾Ð²Ð¾ Ð¸Ð· {positive_words} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð² Ñ‚Ð¾Ð¿-{top_k}")
    if best_neg is None:
        print(f"âš ï¸ ÐÐ¸ Ð¾Ð´Ð½Ð¾ ÑÐ»Ð¾Ð²Ð¾ Ð¸Ð· {negative_words} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð² Ñ‚Ð¾Ð¿-{top_k}")

    print(f"âœ… Top positive: {best_pos} ({top_pos:.2f})")
    print(f"âŒ Top negative: {best_neg} ({top_neg:.2f})")

    return top_pos - top_neg



# ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð»Ð¾Ð³Ð¸Ñ‚Ð¾Ð² Ð¿Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¼Ñƒ ÑÐºÑ€Ñ‹Ñ‚Ð¾Ð¼Ñƒ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸ÑŽ
def get_logits_from_hidden(hidden, target_words, top_k=10000):
    logits = model.lm_head(hidden[0, -1])  # logits Ð´Ð»Ñ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ñ‚Ð¾ÐºÐµÐ½Ð°
    topk = torch.topk(logits, top_k)  # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ð¾Ð¿ top_k Ð»Ð¾Ð³Ð¸Ñ‚Ð¾Ð² Ð¸ Ð¸Ð½Ð´ÐµÐºÑÐ¾Ð²
    top_probs = torch.softmax(topk.values, dim=-1)  # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð»Ð¾Ð³Ð¸Ñ‚Ñ‹ Ð² Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚Ð¸
    top_tokens = tokenizer.batch_decode(topk.indices)  # Ð”ÐµÐºÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½Ð´ÐµÐºÑÑ‹ Ð² Ñ‚Ð¾ÐºÐµÐ½Ñ‹

    result = []
    top_pos = -999999
    best_pos = None 
    for logit, token in zip(topk.values, top_tokens):
        stripped_token = token.strip()

        
        if stripped_token in target_words and logit > top_pos:

            top_pos = logit.item()
            best_pos = stripped_token

    result = best_pos
    return result

# ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ ÑÐºÑ€Ñ‹Ñ‚Ð¾Ð³Ð¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ
def get_last_hidden(prompt):
    start_token_id = tokenizer.bos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(device)


    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
    return outputs.hidden_states[-1]

# ÐŸÐ¾Ð´Ð¼ÐµÐ½Ð° Ð¾Ð´Ð½Ð¾Ð¹ attention Ð³Ð¾Ð»Ð¾Ð²Ñ‹
def patch_single_head(orig, corrupt, head_idx, head_size):
    print("inside patch single head")
    print(orig.shape)
    patched = orig.clone()
    # get the part of attention contributed by that head 
    start = head_idx * head_size
    end = (head_idx + 1) * head_size
    patched[0, -1, start:end] = corrupt[0, -1, start:end]
    print(patched.shape)
    return patched

# Ð—Ð°Ð¿ÑƒÑÐº Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ Ð¿Ð¾Ð´Ð¼ÐµÐ½Ñ‘Ð½Ð½Ñ‹Ð¼ attention Ð²Ñ‹Ñ…Ð¾Ð´Ð¾Ð¼
def run_with_patched_attn(patched_attn, prompt, target_words):
    start_token_id = tokenizer.bos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(device)



    def hook(module, input, output):
        print("printing patched attention!")
        print(patched_attn.shape)
        return (patched_attn, ) + output[1:]


    handle = model.transformer.h[-1].attn.register_forward_hook(hook)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        hidden = outputs.hidden_states[-1]
        logits = get_logits_from_hidden(hidden, target_words)
        print_topk_logits(hidden)  # â† Ð·Ð´ÐµÑÑŒ
    handle.remove()
    
    return logits


def plot_attention(attn_weights, head_idx, tokens, title=None, save_dir="no_bos_attn_images"):
    # Ð•ÑÐ»Ð¸ attn_weights â€” ÐºÐ¾Ñ€Ñ‚ÐµÐ¶, Ð±ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚
    if isinstance(attn_weights, tuple):
        attn_weights = attn_weights[0]

    print(tokens)
    
    if tokens == ["Ð¾Ð½Ð°", "Ð´Ð¾Ñ€Ð¾Ð¶Ð¸Ñ‚"]:
        tokens = ["Ð¾Ð½Ð°", "Ð´Ð¾Ñ€Ð¾", "Ð¶Ð¸Ñ‚"]

    
    print(attn_weights.shape)  # [batch_size, num_heads, seq_len, seq_len]
    attn = attn_weights[0, head_idx].cpu().numpy()  # shape: [seq_len, seq_len]
    print(attn.shape)

    plt.figure(figsize=(8, 6))
    plt.imshow(attn, cmap="viridis")
    plt.colorbar()
    plt.xticks(ticks=range(len(tokens)), labels=tokens, rotation=90)
    plt.yticks(ticks=range(len(tokens)), labels=tokens)
    if title:
        plt.title(title)
    plt.tight_layout()

    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ, ÐµÑÐ»Ð¸ ÐµÑ‘ Ð½ÐµÑ‚
    os.makedirs(save_dir, exist_ok=True)

    # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð¼Ñ Ñ„Ð°Ð¹Ð»Ð°
    token_str = "_".join(tokens).replace("/", "_").replace(" ", "_")[:100]
    filename = f"head{head_idx}_{token_str}.png"
    filepath = os.path.join(save_dir, filename)
    
    plt.savefig(filepath, dpi=300)
    print(f"Saved to {filepath}")
    plt.show()





# ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ†Ð¸ÐºÐ»
n_heads = model.config.n_head
head_size = model.config.n_embd // n_heads




for i, (clean_prompt, corrupt_prompt, correct, wrong) in enumerate(zip(true, corrupted, true_case, corrupted_acc)):
    print(f"\nðŸ§ª ÐŸÑ€Ð¸Ð¼ÐµÑ€ {i+1}: {clean_prompt}")

    print("/n/n CLEAN PROMPT: ", clean_prompt)
    print("CORRUPT PROMPT: ", corrupt_prompt)


    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ attention-Ð²Ñ‹Ñ…Ð¾Ð´Ñ‹
    clean_inputs = get_attention_output(clean_prompt, clean_attn_output) #puts the attention in storage
    print("clean_inputs: ", clean_inputs)
    corrupt_inputs = get_attention_output(corrupt_prompt, corrupt_attn_output)
    clean_attn = clean_attn_output[0]  # [1, seq_len, emb_dim]
    corrupt_attn = corrupt_attn_output[0]

    

    



    # Ð›Ð¾Ð³Ð¸Ñ‚Ñ‹ Ð±ÐµÐ· Ð¿Ð°Ñ‚Ñ‡Ð¸Ð½Ð³Ð°
    with torch.no_grad():
        clean_hidden = get_last_hidden(clean_prompt)
        clean_diff = get_logits_diff_by_category(clean_hidden, true_case, corrupted_acc)
        print(f"âœ… Ð‘ÐµÐ· Ð¿Ð°Ñ‚Ñ‡Ð°: {correct} diff = {clean_diff:.4f}")
        print_topk_logits(clean_hidden)  # ðŸ” Ð²Ð¾Ñ‚ Ñ‚ÑƒÑ‚


    # ÐŸÐµÑ€ÐµÐ±Ð¾Ñ€ Ð³Ð¾Ð»Ð¾Ð²
    head_deltas = []


    for head in range(n_heads):
        print("clean_atten shape!, ", clean_attn.shape)
        patched = patch_single_head(clean_attn, corrupt_attn, head, head_size)
        patched_diff = get_logits_diff_by_category(patched, true_case, corrupted_acc)
        
        delta = patched_diff - clean_diff

        head_deltas.append((head, delta))
        print(f"ðŸ” Head {head:2d}: Patched diff = {patched_diff:.4f}, Î” = {delta:+.4f}")
        print("difference from clean diff: ", patched_diff - clean_diff)

        

        # decoding first token, which seems to recieve a lot of attention

        print("clean_atten shape!, ", clean_attn.shape)
        only_sample = clean_attn[0]
        first_token = only_sample[0]
        print("first token shape!: ", first_token.shape)

        logits = model.lm_head(first_token)  # [vocab_size]


        topk_values, topk_indices = logits.topk(k=5)


        topk_tokens = tokenizer.batch_decode(topk_indices)


        print("top predictions for first token")
        for token, value in zip(topk_tokens, topk_values.tolist()):
            print(f"{token}: {value:.4f}")

    # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð³Ð¾Ð»Ð¾Ð² Ð¿Ð¾ Ð²Ð»Ð¸ÑÐ½Ð¸ÑŽ
    
    head_deltas.sort(key=lambda x: abs(x[1]), reverse=True)
    print("\nprinting for prompt: ", clean_prompt, corrupt_prompt)
    for rank, (head, delta) in enumerate(head_deltas, 1):
        print(f"   {rank}. Head {head:2d}: Î” = {delta:+.4f}")
    
    get_attention_weights(clean_prompt, attn_weights_storage)
    clean_attn_weights = attn_weights_storage[0]
    print("clean attn_weights")
    print(clean_attn_weights.shape)

    tokens = clean_prompt.split(" ")
    for i in range(3):
        if i == 0: 
            t = "best"
        if i == 1:
            t = "second best"
        if i == 2:
            t = "third best"
        head, delta = head_deltas[i]
        print(f"Ð’Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ð´Ð»Ñ Ð³Ð¾Ð»Ð¾Ð²Ñ‹ {head} Ñ Î”={delta:.4f}")
        title = "Clean prompt attention for " + t + "head: " + str(head)
        plot_attention(clean_attn_weights, head, tokens, title=title)



# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# this should be a new file 

# probing classifier for first token 
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split
from tqdm import tqdm

class InstrumentalClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super().__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.dropout = nn.Dropout(0.1)
        self.linear2 = nn.Linear(hidden_dim, 1) 
     
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = self.dropout(x)
        x = self.linear2(x)
        return torch.sigmoid(x)  

# added more instrumental stems 
true = [
    "Ñ Ð³Ð¾Ñ€Ð¶ÑƒÑÑŒ", "Ð¾Ð½Ð° Ð´Ð¾Ñ€Ð¾Ð¶Ð¸Ñ‚", "Ð¼Ñ‹ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼", "Ñ‚Ñ‹ Ð²Ð¾ÑÑ…Ð¸Ñ‰Ð°ÐµÑˆÑŒÑÑ", "Ð¾Ð½Ð¸ Ð²Ð»Ð°Ð´ÐµÑŽÑ‚",
    "Ñ Ð½Ð°ÑÐ»Ð°Ð¶Ð´Ð°ÑŽÑÑŒ", "Ð¾Ð½ ÑƒÐ²Ð»ÐµÑ‡Ñ‘Ð½", "Ð¼Ñ‹ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÑƒÐµÐ¼ÑÑ", "Ñ‚Ñ‹ Ð·Ð°Ð½Ð¸Ð¼Ð°ÐµÑˆÑŒÑÑ", "Ð¾Ð½Ð° Ð´Ð¾Ð²Ð¾Ð»ÑŒÐ½Ð°",
    "Ð¾Ð½ Ð¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ", "Ð¾Ð½Ð¸ ÑˆÑƒÑ‚ÑÑ‚ Ð½Ð°Ð´", "Ñ‚Ñ‹ ÑÐ¼ÐµÑ‘ÑˆÑŒÑÑ Ð½Ð°Ð´", "Ð¼Ñ‹ Ð¸Ð·Ð´ÐµÐ²Ð°ÐµÐ¼ÑÑ Ð½Ð°Ð´", "Ð¾Ð½Ð° ÑƒÑ…Ð°Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ð·Ð°",
    "Ñ Ð½Ð°Ð±Ð»ÑŽÐ´Ð°ÑŽ Ð·Ð°", "Ð¾Ð½ ÑÐ»ÐµÐ´Ð¸Ñ‚ Ð·Ð°", "Ñ‚Ñ‹ Ð·Ð°Ð±Ð¾Ñ‚Ð¸ÑˆÑŒÑÑ Ð¾", "Ð¼Ñ‹ Ð²Ð¾ÑŽÐµÐ¼ Ñ", "Ð¾Ð½Ð¸ ÑÐ¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸Ñ‡Ð°ÑŽÑ‚ Ñ",
    "Ñ Ð±Ð¾Ñ€ÑŽÑÑŒ Ñ", "Ð¾Ð½ Ð´ÐµÐ»Ð¸Ñ‚ÑÑ Ñ", "Ð¾Ð½Ð° ÑÐ¿Ð¾Ñ€Ð¸Ñ‚ Ñ", "Ð¼Ñ‹ ÑÑ€Ð°Ð¶Ð°ÐµÐ¼ÑÑ Ñ", "Ñ‚Ñ‹ Ð´ÐµÑ€Ñ‘ÑˆÑŒÑÑ Ñ",
    "Ð¾Ð½Ð¸ Ð·Ð½Ð°ÐºÐ¾Ð¼ÑÑ‚ÑÑ Ñ", "Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÑŽ Ð½Ð°Ð´", "Ñ‚Ñ‹ Ð·Ð°Ð½Ð¸Ð¼Ð°ÐµÑˆÑŒÑÑ Ñ", "Ð¾Ð½ ÑÐ¿Ð¾Ñ€Ð¸Ñ‚ Ñ", "Ð¾Ð½Ð° ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ",
    "Ð¼Ñ‹ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²ÑƒÐµÐ¼ Ñ", "Ñ Ñ€ÑƒÐ³Ð°ÑŽÑÑŒ Ñ", "Ñ‚Ñ‹ Ð¼Ð¸Ñ€Ð¸ÑˆÑŒÑÑ Ñ", "Ð¾Ð½Ð¸ Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÑ‹Ð²Ð°ÑŽÑ‚ÑÑ Ñ", "Ð¼Ñ‹ ÑÐ¾Ð²ÐµÑ‚ÑƒÐµÐ¼ÑÑ Ñ",
    "Ð¾Ð½ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð¸Ñ€ÑƒÐµÑ‚ÑÑ Ñ", "Ð¾Ð½Ð° Ð¸Ð³Ñ€Ð°ÐµÑ‚ Ñ", "Ñ ÑÐ¾Ñ€ÐµÐ²Ð½ÑƒÑŽÑÑŒ Ñ", "Ñ‚Ñ‹ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð°Ñ€Ð¸Ð²Ð°ÐµÑˆÑŒ Ñ", "Ð¼Ñ‹ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÐµÐ¼ Ñ",
    "Ð¾Ð½Ð¸ Ð¾Ð±Ñ‰Ð°ÑŽÑ‚ÑÑ Ñ", "Ð¾Ð½ Ð´ÐµÐ»Ð¸Ñ‚ÑÑ Ð²Ð¿ÐµÑ‡Ð°Ñ‚Ð»ÐµÐ½Ð¸ÑÐ¼Ð¸ Ñ", "Ñ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÑŽÑÑŒ Ñ", "Ð¾Ð½Ð° Ð¿Ð¾ÑÐ¾Ð²ÐµÑ‚Ð¾Ð²Ð°Ð»Ð°ÑÑŒ Ñ",
    "Ð¼Ñ‹ ÑÐ¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸Ñ‡Ð°ÐµÐ¼ Ñ", "Ñ‚Ñ‹ Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÑ‹Ð²Ð°ÐµÑˆÑŒÑÑ Ñ", "Ð¾Ð½Ð¸ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²ÑƒÑŽÑ‚ Ñ", "Ð¾Ð½ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ",
    "Ñ ÑÐ¾Ð¿ÐµÑ€ÐµÐ¶Ð¸Ð²Ð°ÑŽ Ñ", "Ð¾Ð½Ð° ÑÐ¼ÐµÑ‘Ñ‚ÑÑ Ñ", "Ð¼Ñ‹ Ð±Ð¾Ñ€ÐµÐ¼ÑÑ Ð²Ð¼ÐµÑÑ‚Ðµ Ñ", "Ñ‚Ñ‹ ÑÑ€Ð°Ð¶Ð°ÐµÑˆÑŒÑÑ Ñ€ÑÐ´Ð¾Ð¼ Ñ",
    "Ð¾Ð½Ð¸ Ð¾Ð±Ð¼ÐµÐ½Ð¸Ð²Ð°ÑŽÑ‚ÑÑ Ñ", "Ñ Ð´ÐµÐ»ÑŽÑÑŒ Ð·Ð½Ð°Ð½Ð¸ÑÐ¼Ð¸ Ñ", "Ð¾Ð½ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð°Ñ€Ð¸Ð²Ð°ÐµÑ‚ Ñ", "Ð¾Ð½Ð° ÑˆÑƒÑ‚Ð¸Ñ‚ Ð½Ð°Ð´",
    "Ð¼Ñ‹ Ð½Ð°ÑÐ¼ÐµÑ…Ð°ÐµÐ¼ÑÑ Ð½Ð°Ð´", "Ñ‚Ñ‹ Ð·Ð°Ð±Ð¾Ñ‚Ð¸ÑˆÑŒÑÑ Ð¾", "Ð¾Ð½Ð¸ ÑƒÑ…Ð°Ð¶Ð¸Ð²Ð°ÑŽÑ‚ Ð·Ð°", "Ñ Ð²Ð¾ÑŽÑŽ Ñ", "Ð¾Ð½ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð¸Ñ€ÑƒÐµÑ‚ÑÑ Ñƒ",
    "Ð¾Ð½Ð° Ð¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ", "Ð¼Ñ‹ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼", "Ñ‚Ñ‹ Ð³Ð¾Ñ€Ð´Ð¸ÑˆÑŒÑÑ", "Ð¾Ð½Ð¸ Ð·Ð°Ð½Ð¸Ð¼Ð°ÑŽÑ‚ÑÑ", "Ñ Ð´Ð¾Ð²Ð¾Ð»ÐµÐ½",
    "Ð¾Ð½ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÑƒÐµÑ‚ÑÑ", "Ð¾Ð½Ð° Ð²Ð¾ÑÑ…Ð¸Ñ‰Ð°ÐµÑ‚ÑÑ", "Ð¼Ñ‹ Ð²Ð»Ð°Ð´ÐµÐµÐ¼", "Ñ‚Ñ‹ Ð½Ð°ÑÐ»Ð°Ð¶Ð´Ð°ÐµÑˆÑŒÑÑ", "Ð¾Ð½Ð¸ Ð´Ð¾Ñ€Ð¾Ð¶Ð°Ñ‚",
    "Ñ ÑƒÐ²Ð»ÐµÐºÐ°ÑŽÑÑŒ", "Ð¾Ð½ ÑƒÐ²Ð»ÐµÐºÐ°ÐµÑ‚ÑÑ", "Ð¾Ð½Ð° Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²ÑƒÐµÑ‚ Ñ", "Ð¼Ñ‹ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ñ", "Ñ‚Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑˆÑŒ Ð½Ð°Ð´",
    "Ð¾Ð½Ð¸ ÑÑ‚Ñ€Ð¾ÑÑ‚ Ñ", "Ñ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÑŽ Ñ", "Ð¾Ð½ ÑÐ¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸Ñ‡Ð°ÑŽÑ‚ Ñ", "Ð¾Ð½Ð° Ð´ÐµÐ»Ð¸Ñ‚ÑÑ Ð¾Ð¿Ñ‹Ñ‚Ð¾Ð¼ Ñ",
    "Ð¼Ñ‹ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²ÑƒÐµÐ¼", "Ñ‚Ñ‹ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÐµÑˆÑŒ Ñ", "Ð¾Ð½Ð¸ Ð´ÐµÐ»ÑÑ‚ÑÑ Ð¼Ð½ÐµÐ½Ð¸ÐµÐ¼ Ñ", "Ñ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚ Ñ",
    "Ð¾Ð½ ÑˆÑƒÑ‚Ð¸Ñ‚ Ñ", "Ð¾Ð½Ð° ÑÐ¾Ð²ÐµÑ‚ÑƒÐµÑ‚ÑÑ Ñ", "Ð¼Ñ‹ ÑÐ¼ÐµÑ‘Ð¼ÑÑ Ð½Ð°Ð´", "Ñ‚Ñ‹ Ñ€Ð°ÑÑÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑˆÑŒ Ð¾", "Ð¾Ð½Ð¸ Ð´ÐµÐ»ÑÑ‚ÑÑ Ð¼Ñ‹ÑÐ»ÑÐ¼Ð¸ Ñ",
    "Ñ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²ÑƒÑŽ Ñ", "Ð¾Ð½ Ð²Ð·Ð°Ð¸Ð¼Ð¾Ð´ÐµÐ¹ÑÑ‚Ð²ÑƒÐµÑ‚ Ñ", "Ð¾Ð½Ð° ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¸Ñ€ÑƒÐµÑ‚ Ñ", "Ð¼Ñ‹ ÑÑ€Ð°Ð¶Ð°ÐµÐ¼ÑÑ Ñ€ÑÐ´Ð¾Ð¼ Ñ",
    "Ñ‚Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑˆÑŒ Ð²Ð¼ÐµÑÑ‚Ðµ Ñ", "Ð¾Ð½Ð¸ Ð±Ð¾Ñ€ÑŽÑ‚ÑÑ Ñ", "Ñ Ñ€ÑƒÐ³Ð°ÑŽÑÑŒ Ð½Ð°", "Ð¾Ð½ Ð·Ð°Ð½Ð¸Ð¼Ð°ÐµÑ‚ÑÑ ÑÐ¿Ð¾Ñ€Ñ‚Ð¾Ð¼ Ñ",
    "Ð¾Ð½Ð° Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÐµÑ‚ Ñ", "Ð¼Ñ‹ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÐµÐ¼ Ð²Ð¼ÐµÑÑ‚Ðµ Ñ", "Ñ‚Ñ‹ Ð´ÐµÐ»Ð¸ÑˆÑŒÑÑ Ñ", "Ð¾Ð½Ð¸ Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÑ‹Ð²Ð°ÑŽÑ‚ÑÑ Ñ"
]

false = [
    "Ñ Ð»ÑŽÐ±Ð»ÑŽ", "Ð¼Ñ‹ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°ÐµÐ¼", "Ð¾Ð½Ð° Ð²Ð¸Ð´Ð¸Ñ‚", "Ð¾Ð½Ð¸ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÑŽÑ‚",
    "Ñ Ñ…Ð¾Ñ‡Ñƒ", "Ñ‚Ñ‹ ÑÐ»ÑƒÑˆÐ°ÐµÑˆÑŒ", "Ð¾Ð½ Ð¸Ð·ÑƒÑ‡Ð°ÐµÑ‚", "Ð¼Ñ‹ Ð¾ÑÑƒÐ¶Ð´Ð°ÐµÐ¼", "Ð¾Ð½Ð¸ Ñ‚ÐµÑ€Ð¿ÑÑ‚",
    "Ð¾Ð½Ð° Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°ÐµÑ‚", "Ñ‚Ñ‹ Ñ‡Ð¸Ñ‚Ð°ÐµÑˆÑŒ", "Ñ Ð·Ð½Ð°ÑŽ", "Ð¼Ñ‹ Ð¿Ð¾Ð¼Ð½Ð¸Ð¼", "Ð¾Ð½ Ð·Ð°Ð±Ñ‹Ð²Ð°ÐµÑ‚",
    "Ñ‚Ñ‹ Ð¶Ð´Ñ‘ÑˆÑŒ", "Ð¾Ð½Ð° Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚", "Ð¾Ð½Ð¸ Ð±Ñ€Ð¾ÑÐ°ÑŽÑ‚", "Ð¼Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼", "Ð¾Ð½ Ð·Ð°Ð¼ÐµÑ‡Ð°ÐµÑ‚",
    "Ñ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÑŽ", "Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð±ÑƒÐµÑˆÑŒ", "Ð¾Ð½Ð° ÑÑ‚Ñ€Ð¾Ð¸Ñ‚", "Ð¼Ñ‹ ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼", "Ð¾Ð½Ð¸ Ð¿Ð¾ÐºÑƒÐ¿Ð°ÑŽÑ‚",
    "Ñ‚Ñ‹ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÑˆÑŒ", "Ð¾Ð½Ð° Ð¿Ð¸ÑˆÐµÑ‚", "Ñ Ð½Ð°Ñ…Ð¾Ð¶Ñƒ", "Ð¾Ð½ Ð»Ð¾Ð²Ð¸Ñ‚", "Ð¼Ñ‹ Ð²Ñ‹Ð½Ð¾ÑÐ¸Ð¼",
    "Ð¾Ð½Ð¸ Ñ‡Ð¸Ñ‚Ð°ÑŽÑ‚", "Ð¾Ð½Ð° ÑÐ»ÑƒÑˆÐ°ÐµÑ‚", "Ñ‚Ñ‹ Ð²Ð¸Ð´Ð¸ÑˆÑŒ", "Ñ ÑÐ»Ñ‹ÑˆÑƒ", "Ð¾Ð½ Ñ…Ð¾Ñ‡ÐµÑ‚",

]

model.eval()

@torch.no_grad()
def get_first_token_embedding(sentences):
    embeddings = []
    for sent in tqdm(sentences):
        inputs = tokenizer(sent, return_tensors="pt").to(device)
        outputs = model(**inputs, output_hidden_states=True)
        hidden_states = outputs.hidden_states[-1]  
        first_token_embed = hidden_states[:, 0, :]  
        embeddings.append(first_token_embed.squeeze(0).cpu())
    return torch.stack(embeddings)

X_true = get_first_token_embedding(true)
X_false = get_first_token_embedding(false)

X = torch.cat([X_true, X_false], dim=0)
y = torch.cat([
    torch.ones(len(X_true)),   
    torch.zeros(len(X_false)) 
], dim=0)

dataset = TensorDataset(X, y)
n_total = len(dataset)
n_test = int(n_total * 0.15)
n_train = n_total - n_test
train_dataset, test_dataset = random_split(dataset, [n_train, n_test], generator=torch.Generator().manual_seed(42))

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)

clf = InstrumentalClassifier(input_dim=X.shape[1]).to(device)
loss_fn = nn.BCELoss()
optimizer = torch.optim.Adam(clf.parameters(), lr=1e-3)

for epoch in range(3):
    total_loss = 0
    clf.train()
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device).unsqueeze(1)
        optimizer.zero_grad()
        preds = clf(batch_x)
        loss = loss_fn(preds, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}: loss = {total_loss:.4f}")

clf.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch_x, batch_y in test_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        preds = clf(batch_x).squeeze()
        predicted = (preds > 0.5).long()
        correct += (predicted == batch_y.long()).sum().item()
        total += batch_y.size(0)

print(f"Test accuracy: {correct / total:.2%}")