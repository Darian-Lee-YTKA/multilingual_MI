import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch.nn.functional as F
import matplotlib
import matplotlib.pyplot as plt
import os
# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏ –º–æ–¥–µ–ª—å
device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
model_name = "sberbank-ai/rugpt3large_based_on_gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(device)
model.eval()

# –î–∞–Ω–Ω—ã–µ
print("tokeizer")
print(tokenizer.tokenize("—Å–≤–æ–µ–π"))
print(tokenizer.tokenize("—Ç–µ–±—è"))
print(tokenizer.tokenize("–µ–≥–æ"))
print(tokenizer.tokenize("—Å–≤–æ–∏–º"))

true = ["–æ–Ω–∞ –¥–æ—Ä–æ–∂–∏—Ç", "—è –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞—Å—å —Å"]
true_case = ["–ø–∞—Ä–Ω–µ–º", "—Å–≤–æ–µ–π", "—Å–≤–æ–∏–º", "–Ω–∏–º", "—Ç–µ–º", "–º—É–∂—á–∏–Ω–æ–π", "–¥–µ–≤—É—à–∫–æ–π", "—á–µ–ª–æ–≤–µ–∫–æ–º", "—Ç–æ–±–æ–π", "–∏–º", "—ç—Ç–∏–º", "–≤–∞–º–∏"]
corrupted_acc = ["—Ç–µ–±—è", "–µ–≥–æ", "—Ç–æ–≥–æ", "–≤–æ–ø—Ä–æ—Å–∞", "–º–µ–Ω—è", "–º–∞–ª–æ–≥–æ", "—Å–µ–±—è"]
corrupted = ["–æ–Ω–∞ –æ–±–æ–∂–∞–µ—Ç", "—è –Ω–∞—á–∞–ª–∞ —Å"]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
clean_attn_output = []
corrupt_attn_output = []

attn_weights_storage = []

def get_attention_weights(prompt, storage):
    storage.clear()
    start_token_id = tokenizer.bos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    if start_token_id is not None:
        start_token_tensor = torch.tensor([[start_token_id]], device=device)
        input_ids = torch.cat([start_token_tensor, inputs["input_ids"]], dim=1)

        if "attention_mask" in inputs:
            start_mask = torch.ones((inputs["attention_mask"].shape[0], 1), device=device, dtype=inputs["attention_mask"].dtype)
            attention_mask = torch.cat([start_mask, inputs["attention_mask"]], dim=1)
        else:
            attention_mask = torch.ones_like(input_ids)
    else:
        input_ids = inputs["input_ids"]
        attention_mask = inputs.get("attention_mask", torch.ones_like(input_ids))
    print("LEN INPUTS:", len(inputs))
    print("MI_INPUTS:,", inputs)
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)

    attentions = outputs.attentions  
    last_layer_attn = attentions[-1].detach().cpu()  
    storage.append(last_layer_attn)



def print_topk_logits(hidden, k=5):
    logits = model.lm_head(hidden[0, -1])
    topk = torch.topk(logits, k)
    tokens = tokenizer.batch_decode(topk.indices)
    print("üîù Top-5 predictions:")
    for token, score in zip(tokens, topk.values):
        print(f"   {token.strip():>10} ({score.item():.2f})")


def get_attention_output(prompt, storage):
    storage.clear()
    start_token_id = tokenizer.bos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    if start_token_id is not None:
        start_token_tensor = torch.tensor([[start_token_id]], device=device)
        input_ids = torch.cat([start_token_tensor, inputs["input_ids"]], dim=1)

        if "attention_mask" in inputs:
            start_mask = torch.ones((inputs["attention_mask"].shape[0], 1), device=device, dtype=inputs["attention_mask"].dtype)
            attention_mask = torch.cat([start_mask, inputs["attention_mask"]], dim=1)
        else:
            attention_mask = torch.ones_like(input_ids)
    else:
        input_ids = inputs["input_ids"]
        attention_mask = inputs.get("attention_mask", torch.ones_like(input_ids))

    def hook(module, input, output):
        # grabs attention output and stores it in storage 
        
        atten =  output[0].detach()
        print("shape of attention output!: ", atten.shape)
        storage.append(atten)

    # attachs hook to the last layer of attention (identified as important through logit lens)
    handle = model.transformer.h[-1].attn.register_forward_hook(hook)
    with torch.no_grad():
        # run foward pass. hook grabs the attention
        _ = model(input_ids=input_ids, attention_mask=attention_mask)
    # remove hook
    handle.remove()
    return inputs




def get_logits_diff_by_category(hidden, positive_words, negative_words, top_k=10000):
    logits = model.lm_head(hidden[0, -1])
    topk = logits.topk(k=top_k, dim=-1) 
    tokens = tokenizer.batch_decode(topk.indices)

    top_pos = float('-inf')
    top_neg = float('-inf')
    best_pos = None
    best_neg = None

    print(positive_words)
    print(negative_words)
    for logit, token in zip(topk.values, tokens):
        stripped_token = token.strip()

        
        if stripped_token in positive_words and logit > top_pos:

            top_pos = logit.item()
            best_pos = stripped_token
        elif stripped_token in negative_words and logit > top_neg:
            top_neg = logit.item()
            best_neg = stripped_token

    if best_pos is None:
        print(f"‚ö†Ô∏è –ù–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏–∑ {positive_words} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-{top_k}")
    if best_neg is None:
        print(f"‚ö†Ô∏è –ù–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏–∑ {negative_words} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-{top_k}")

    print(f"‚úÖ Top positive: {best_pos} ({top_pos:.2f})")
    print(f"‚ùå Top negative: {best_neg} ({top_neg:.2f})")

    return top_pos - top_neg



# –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–∫—Ä—ã—Ç–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é
def get_logits_from_hidden(hidden, target_words, top_k=10000):
    logits = model.lm_head(hidden[0, -1])  # logits –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
    topk = torch.topk(logits, top_k)  # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø top_k –ª–æ–≥–∏—Ç–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–æ–≤
    top_probs = torch.softmax(topk.values, dim=-1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ª–æ–≥–∏—Ç—ã –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    top_tokens = tokenizer.batch_decode(topk.indices)  # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ —Ç–æ–∫–µ–Ω—ã

    result = []
    top_pos = -999999
    best_pos = None 
    for logit, token in zip(topk.values, top_tokens):
        stripped_token = token.strip()

        
        if stripped_token in target_words and logit > top_pos:

            top_pos = logit.item()
            best_pos = stripped_token

    result = best_pos
    return result

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
def get_last_hidden(prompt):
    start_token_id = tokenizer.bos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    if start_token_id is not None:
        start_token_tensor = torch.tensor([[start_token_id]], device=device)
        input_ids = torch.cat([start_token_tensor, inputs["input_ids"]], dim=1)

        if "attention_mask" in inputs:
            start_mask = torch.ones((inputs["attention_mask"].shape[0], 1), device=device, dtype=inputs["attention_mask"].dtype)
            attention_mask = torch.cat([start_mask, inputs["attention_mask"]], dim=1)
        else:
            attention_mask = torch.ones_like(input_ids)
    else:
        input_ids = inputs["input_ids"]
        attention_mask = inputs.get("attention_mask", torch.ones_like(input_ids))
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
    return outputs.hidden_states[-1]

# –ü–æ–¥–º–µ–Ω–∞ –æ–¥–Ω–æ–π attention –≥–æ–ª–æ–≤—ã
def patch_single_head(orig, corrupt, head_idx, head_size):
    patched = orig.clone()
    start = head_idx * head_size
    end = (head_idx + 1) * head_size
    patched[0, -1, start:end] = corrupt[0, -1, start:end]
    return patched

# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–º–µ–Ω—ë–Ω–Ω—ã–º attention –≤—ã—Ö–æ–¥–æ–º
def run_with_patched_attn(patched_attn, prompt, target_words):
    start_token_id = tokenizer.bos_token_id
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    if start_token_id is not None:
        start_token_tensor = torch.tensor([[start_token_id]], device=device)
        input_ids = torch.cat([start_token_tensor, inputs["input_ids"]], dim=1)

        if "attention_mask" in inputs:
            start_mask = torch.ones((inputs["attention_mask"].shape[0], 1), device=device, dtype=inputs["attention_mask"].dtype)
            attention_mask = torch.cat([start_mask, inputs["attention_mask"]], dim=1)
        else:
            attention_mask = torch.ones_like(input_ids)
    else:
        input_ids = inputs["input_ids"]
        attention_mask = inputs.get("attention_mask", torch.ones_like(input_ids))

    def hook(module, input, output):
        print("printing patched attention!")
        print(patched_attn.shape)
        return (patched_attn, ) + output[1:]


    handle = model.transformer.h[-1].attn.register_forward_hook(hook)
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)
        hidden = outputs.hidden_states[-1]
        logits = get_logits_from_hidden(hidden, target_words)
        print_topk_logits(hidden)  # ‚Üê –∑–¥–µ—Å—å
    handle.remove()
    
    return logits


def plot_attention(attn_weights, head_idx, tokens, title=None, save_dir="attn_images"):
    # –ï—Å–ª–∏ attn_weights ‚Äî –∫–æ—Ä—Ç–µ–∂, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
    if isinstance(attn_weights, tuple):
        attn_weights = attn_weights[0]

    print(tokens)
    
    if tokens == ["–æ–Ω–∞", "–¥–æ—Ä–æ–∂–∏—Ç"]:
        tokens = ["–æ–Ω–∞", "–¥–æ—Ä–æ", "–∂–∏—Ç"]
    tokens = ["<BOS>"] + tokens
    
    print(attn_weights.shape)  # [batch_size, num_heads, seq_len, seq_len]
    attn = attn_weights[0, head_idx].cpu().numpy()  # shape: [seq_len, seq_len]
    print(attn.shape)

    plt.figure(figsize=(8, 6))
    plt.imshow(attn, cmap="viridis")
    plt.colorbar()
    plt.xticks(ticks=range(len(tokens)), labels=tokens, rotation=90)
    plt.yticks(ticks=range(len(tokens)), labels=tokens)
    if title:
        plt.title(title)
    plt.tight_layout()

    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    os.makedirs(save_dir, exist_ok=True)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    token_str = "_".join(tokens).replace("/", "_").replace(" ", "_")[:100]
    filename = f"head{head_idx}_{token_str}.png"
    filepath = os.path.join(save_dir, filename)
    
    plt.savefig(filepath, dpi=300)
    print(f"Saved to {filepath}")
    plt.show()





# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
n_heads = model.config.n_head
head_size = model.config.n_embd // n_heads




for i, (clean_prompt, corrupt_prompt, correct, wrong) in enumerate(zip(true, corrupted, true_case, corrupted_acc)):
    print(f"\nüß™ –ü—Ä–∏–º–µ—Ä {i+1}: {clean_prompt}")

    print("/n/n CLEAN PROMPT: ", clean_prompt)
    print("CORRUPT PROMPT: ", corrupt_prompt)


    # –ü–æ–ª—É—á–∞–µ–º attention-–≤—ã—Ö–æ–¥—ã
    clean_inputs = get_attention_output(clean_prompt, clean_attn_output) #puts the attention in storage
    print("clean_inputs: ", clean_inputs)
    corrupt_inputs = get_attention_output(corrupt_prompt, corrupt_attn_output)
    clean_attn = clean_attn_output[0]  # [1, seq_len, emb_dim]
    corrupt_attn = corrupt_attn_output[0]

    # –õ–æ–≥–∏—Ç—ã –±–µ–∑ –ø–∞—Ç—á–∏–Ω–≥–∞
    with torch.no_grad():
        clean_hidden = get_last_hidden(clean_prompt)
        clean_diff = get_logits_diff_by_category(clean_hidden, true_case, corrupted_acc)
        print(f"‚úÖ –ë–µ–∑ –ø–∞—Ç—á–∞: {correct} diff = {clean_diff:.4f}")
        print_topk_logits(clean_hidden)  # üîç –≤–æ—Ç —Ç—É—Ç


    # –ü–µ—Ä–µ–±–æ—Ä –≥–æ–ª–æ–≤
    head_deltas = []


    for head in range(n_heads):
        print("clean_atten shape!, ", clean_attn.shape)
        patched = patch_single_head(clean_attn, corrupt_attn, head, head_size)
        patched_diff = get_logits_diff_by_category(patched, true_case, corrupted_acc)
        
        delta = patched_diff - clean_diff

        head_deltas.append((head, delta))
        print(f"üîÅ Head {head:2d}: Patched diff = {patched_diff:.4f}, Œî = {delta:+.4f}")
        print("difference from clean diff: ", patched_diff - clean_diff)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≥–æ–ª–æ–≤ –ø–æ –≤–ª–∏—è–Ω–∏—é
    
    head_deltas.sort(key=lambda x: abs(x[1]), reverse=True)
    print("\nprinting for prompt: ", clean_prompt, corrupt_prompt)
    for rank, (head, delta) in enumerate(head_deltas, 1):
        print(f"   {rank}. Head {head:2d}: Œî = {delta:+.4f}")
    
    get_attention_weights(clean_prompt, attn_weights_storage)
    clean_attn_weights = attn_weights_storage[0]
    print("clean attn_weights")
    print(clean_attn_weights.shape)

    tokens = clean_prompt.split(" ")
    for i in range(3):
        if i == 0: 
            t = "best"
        if i == 1:
            t = "second best"
        if i == 2:
            t = "third best"
        head, delta = head_deltas[i]
        print(f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≥–æ–ª–æ–≤—ã {head} —Å Œî={delta:.4f}")
        title = "Clean prompt attention for " + t + "head: " + str(head)
        plot_attention(clean_attn_weights, head, tokens, title=title)


# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# this should be a new file 

# probing classifier for first token 
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split
from tqdm import tqdm

class InstrumentalClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super().__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        self.dropout = nn.Dropout(0.1)
        self.linear2 = nn.Linear(hidden_dim, 1) 
     
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = self.dropout(x)
        x = self.linear2(x)
        return torch.sigmoid(x)  

# added more instrumental stems 
true = [
    "—è –≥–æ—Ä–∂—É—Å—å", "–æ–Ω–∞ –¥–æ—Ä–æ–∂–∏—Ç", "–º—ã —É–ø—Ä–∞–≤–ª—è–µ–º", "—Ç—ã –≤–æ—Å—Ö–∏—â–∞–µ—à—å—Å—è", "–æ–Ω–∏ –≤–ª–∞–¥–µ—é—Ç",
    "—è –Ω–∞—Å–ª–∞–∂–¥–∞—é—Å—å", "–æ–Ω —É–≤–ª–µ—á—ë–Ω", "–º—ã –∏–Ω—Ç–µ—Ä–µ—Å—É–µ–º—Å—è", "—Ç—ã –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è", "–æ–Ω–∞ –¥–æ–≤–æ–ª—å–Ω–∞",
    "–æ–Ω –ø–æ–ª—å–∑—É–µ—Ç—Å—è", "–æ–Ω–∏ —à—É—Ç—è—Ç –Ω–∞–¥", "—Ç—ã —Å–º–µ—ë—à—å—Å—è –Ω–∞–¥", "–º—ã –∏–∑–¥–µ–≤–∞–µ–º—Å—è –Ω–∞–¥", "–æ–Ω–∞ —É—Ö–∞–∂–∏–≤–∞–µ—Ç –∑–∞",
    "—è –Ω–∞–±–ª—é–¥–∞—é –∑–∞", "–æ–Ω —Å–ª–µ–¥–∏—Ç –∑–∞", "—Ç—ã –∑–∞–±–æ—Ç–∏—à—å—Å—è –æ", "–º—ã –≤–æ—é–µ–º —Å", "–æ–Ω–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—Ç —Å",
    "—è –±–æ—Ä—é—Å—å —Å", "–æ–Ω –¥–µ–ª–∏—Ç—Å—è —Å", "–æ–Ω–∞ —Å–ø–æ—Ä–∏—Ç —Å", "–º—ã —Å—Ä–∞–∂–∞–µ–º—Å—è —Å", "—Ç—ã –¥–µ—Ä—ë—à—å—Å—è —Å",
    "–æ–Ω–∏ –∑–Ω–∞–∫–æ–º—è—Ç—Å—è —Å", "—è —Ä–∞–±–æ—Ç–∞—é –Ω–∞–¥", "—Ç—ã –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è —Å", "–æ–Ω —Å–ø–æ—Ä–∏—Ç —Å", "–æ–Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç–∏—Ä—É–µ—Ç —Å",
    "–º—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ–º —Å", "—è —Ä—É–≥–∞—é—Å—å —Å", "—Ç—ã –º–∏—Ä–∏—à—å—Å—è —Å", "–æ–Ω–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è —Å", "–º—ã —Å–æ–≤–µ—Ç—É–µ–º—Å—è —Å",
    "–æ–Ω –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç—Å—è —Å", "–æ–Ω–∞ –∏–≥—Ä–∞–µ—Ç —Å", "—è —Å–æ—Ä–µ–≤–Ω—É—é—Å—å —Å", "—Ç—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å —Å", "–º—ã –æ–±—Å—É–∂–¥–∞–µ–º —Å",
    "–æ–Ω–∏ –æ–±—â–∞—é—Ç—Å—è —Å", "–æ–Ω –¥–µ–ª–∏—Ç—Å—è –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏—è–º–∏ —Å", "—è –≤—Å—Ç—Ä–µ—á–∞—é—Å—å —Å", "–æ–Ω–∞ –ø–æ—Å–æ–≤–µ—Ç–æ–≤–∞–ª–∞—Å—å —Å",
    "–º—ã —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º —Å", "—Ç—ã –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—à—å—Å—è —Å", "–æ–Ω–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é—Ç —Å", "–æ–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–µ—Ç —Å",
    "—è —Å–æ–ø–µ—Ä–µ–∂–∏–≤–∞—é —Å", "–æ–Ω–∞ —Å–º–µ—ë—Ç—Å—è —Å", "–º—ã –±–æ—Ä–µ–º—Å—è –≤–º–µ—Å—Ç–µ —Å", "—Ç—ã —Å—Ä–∞–∂–∞–µ—à—å—Å—è —Ä—è–¥–æ–º —Å",
    "–æ–Ω–∏ –æ–±–º–µ–Ω–∏–≤–∞—é—Ç—Å—è —Å", "—è –¥–µ–ª—é—Å—å –∑–Ω–∞–Ω–∏—è–º–∏ —Å", "–æ–Ω —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—Ç —Å", "–æ–Ω–∞ —à—É—Ç–∏—Ç –Ω–∞–¥",
    "–º—ã –Ω–∞—Å–º–µ—Ö–∞–µ–º—Å—è –Ω–∞–¥", "—Ç—ã –∑–∞–±–æ—Ç–∏—à—å—Å—è –æ", "–æ–Ω–∏ —É—Ö–∞–∂–∏–≤–∞—é—Ç –∑–∞", "—è –≤–æ—é—é —Å", "–æ–Ω –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—Ç—Å—è —É",
    "–æ–Ω–∞ –ø–æ–ª—å–∑—É–µ—Ç—Å—è", "–º—ã —É–ø—Ä–∞–≤–ª—è–µ–º", "—Ç—ã –≥–æ—Ä–¥–∏—à—å—Å—è", "–æ–Ω–∏ –∑–∞–Ω–∏–º–∞—é—Ç—Å—è", "—è –¥–æ–≤–æ–ª–µ–Ω",
    "–æ–Ω –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è", "–æ–Ω–∞ –≤–æ—Å—Ö–∏—â–∞–µ—Ç—Å—è", "–º—ã –≤–ª–∞–¥–µ–µ–º", "—Ç—ã –Ω–∞—Å–ª–∞–∂–¥–∞–µ—à—å—Å—è", "–æ–Ω–∏ –¥–æ—Ä–æ–∂–∞—Ç",
    "—è —É–≤–ª–µ–∫–∞—é—Å—å", "–æ–Ω —É–≤–ª–µ–∫–∞–µ—Ç—Å—è", "–æ–Ω–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å", "–º—ã –∫–æ–Ω—Ç–∞–∫—Ç–∏—Ä—É–µ–º —Å", "—Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –Ω–∞–¥",
    "–æ–Ω–∏ —Å—Ç—Ä–æ—è—Ç —Å", "—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É—é —Å", "–æ–Ω —Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞—é—Ç —Å", "–æ–Ω–∞ –¥–µ–ª–∏—Ç—Å—è –æ–ø—ã—Ç–æ–º —Å",
    "–º—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ–º", "—Ç—ã –æ–±—Å—É–∂–¥–∞–µ—à—å —Å", "–æ–Ω–∏ –¥–µ–ª—è—Ç—Å—è –º–Ω–µ–Ω–∏–µ–º —Å", "—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é –∫–æ–Ω—Ç–∞–∫—Ç —Å",
    "–æ–Ω —à—É—Ç–∏—Ç —Å", "–æ–Ω–∞ —Å–æ–≤–µ—Ç—É–µ—Ç—Å—è —Å", "–º—ã —Å–º–µ—ë–º—Å—è –Ω–∞–¥", "—Ç—ã —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—à—å –æ", "–æ–Ω–∏ –¥–µ–ª—è—Ç—Å—è –º—ã—Å–ª—è–º–∏ —Å",
    "—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É—é —Å", "–æ–Ω –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç —Å", "–æ–Ω–∞ –∫–æ–Ω—Ç–∞–∫—Ç–∏—Ä—É–µ—Ç —Å", "–º—ã —Å—Ä–∞–∂–∞–µ–º—Å—è —Ä—è–¥–æ–º —Å",
    "—Ç—ã —Ä–∞–±–æ—Ç–∞–µ—à—å –≤–º–µ—Å—Ç–µ —Å", "–æ–Ω–∏ –±–æ—Ä—é—Ç—Å—è —Å", "—è —Ä—É–≥–∞—é—Å—å –Ω–∞", "–æ–Ω –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Å–ø–æ—Ä—Ç–æ–º —Å",
    "–æ–Ω–∞ –æ–±—Å—É–∂–¥–∞–µ—Ç —Å", "–º—ã –æ–±—Å—É–∂–¥–∞–µ–º –≤–º–µ—Å—Ç–µ —Å", "—Ç—ã –¥–µ–ª–∏—à—å—Å—è —Å", "–æ–Ω–∏ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è —Å"
]

false = [
    "—è –ª—é–±–ª—é", "–º—ã –æ–±—Å—É–∂–¥–∞–µ–º", "–æ–Ω–∞ –≤–∏–¥–∏—Ç", "–æ–Ω–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç",
    "—è —Ö–æ—á—É", "—Ç—ã —Å–ª—É—à–∞–µ—à—å", "–æ–Ω –∏–∑—É—á–∞–µ—Ç", "–º—ã –æ—Å—É–∂–¥–∞–µ–º", "–æ–Ω–∏ —Ç–µ—Ä–ø—è—Ç",
    "–æ–Ω–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç", "—Ç—ã —á–∏—Ç–∞–µ—à—å", "—è –∑–Ω–∞—é", "–º—ã –ø–æ–º–Ω–∏–º", "–æ–Ω –∑–∞–±—ã–≤–∞–µ—Ç",
    "—Ç—ã –∂–¥—ë—à—å", "–æ–Ω–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç", "–æ–Ω–∏ –±—Ä–æ—Å–∞—é—Ç", "–º—ã –ø–æ–ª—É—á–∞–µ–º", "–æ–Ω –∑–∞–º–µ—á–∞–µ—Ç",
    "—è –≤—ã–±–∏—Ä–∞—é", "—Ç—ã –ø—Ä–æ–±—É–µ—à—å", "–æ–Ω–∞ —Å—Ç—Ä–æ–∏—Ç", "–º—ã —Å–æ–∑–¥–∞—ë–º", "–æ–Ω–∏ –ø–æ–∫—É–ø–∞—é—Ç",
    "—Ç—ã –ø—Ä–æ–≤–µ—Ä—è–µ—à—å", "–æ–Ω–∞ –ø–∏—à–µ—Ç", "—è –Ω–∞—Ö–æ–∂—É", "–æ–Ω –ª–æ–≤–∏—Ç", "–º—ã –≤—ã–Ω–æ—Å–∏–º",
    "–æ–Ω–∏ —á–∏—Ç–∞—é—Ç", "–æ–Ω–∞ —Å–ª—É—à–∞–µ—Ç", "—Ç—ã –≤–∏–¥–∏—à—å", "—è —Å–ª—ã—à—É", "–æ–Ω —Ö–æ—á–µ—Ç",
]

true = [tokenizer.bos_token + " " + s for s in true]
false = [tokenizer.bos_token + " " + s for s in false]

model.eval()

@torch.no_grad()
def get_first_token_embedding(sentences):
    embeddings = []
    for sent in tqdm(sentences):
        inputs = tokenizer(sent, return_tensors="pt").to(device)
        # –î–æ–±–∞–≤–ª—è–µ–º BOS —Ç–æ–∫–µ–Ω –≤ –Ω–∞—á–∞–ª–æ input_ids
        bos_id = tokenizer.bos_token_id
        if bos_id is None:
            raise ValueError("Tokenizer has no bos_token_id")
        input_ids = inputs.input_ids
        # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π tensor —Å BOS –≤ –Ω–∞—á–∞–ª–µ
        input_ids_with_bos = torch.cat([
            torch.tensor([[bos_id]], device=device),
            input_ids
        ], dim=1)
        inputs['input_ids'] = input_ids_with_bos

        # –¢–∞–∫–∂–µ –µ—Å–ª–∏ –µ—Å—Ç—å attention_mask, –¥–æ–±–∞–≤–∏–º —Ç—É–¥–∞ 1 –≤ –Ω–∞—á–∞–ª–µ
        if 'attention_mask' in inputs:
            attention_mask = inputs['attention_mask']
            attention_mask_with_bos = torch.cat([
                torch.tensor([[1]], device=device),
                attention_mask
            ], dim=1)
            inputs['attention_mask'] = attention_mask_with_bos

        outputs = model(**inputs, output_hidden_states=True)
        hidden_states = outputs.hidden_states[-1]  
        first_token_embed = hidden_states[:, 0, :]  
        embeddings.append(first_token_embed.squeeze(0).cpu())
    return torch.stack(embeddings)


X_true = get_first_token_embedding(true)
X_false = get_first_token_embedding(false)

X = torch.cat([X_true, X_false], dim=0)
y = torch.cat([
    torch.ones(len(X_true)),   
    torch.zeros(len(X_false)) 
], dim=0)

dataset = TensorDataset(X, y)
n_total = len(dataset)
n_test = int(n_total * 0.15)
n_train = n_total - n_test
train_dataset, test_dataset = random_split(dataset, [n_train, n_test], generator=torch.Generator().manual_seed(42))

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8)

clf = InstrumentalClassifier(input_dim=X.shape[1]).to(device)
loss_fn = nn.BCELoss()
optimizer = torch.optim.Adam(clf.parameters(), lr=1e-3)

for epoch in range(3):
    total_loss = 0
    clf.train()
    for batch_x, batch_y in train_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device).unsqueeze(1)
        optimizer.zero_grad()
        preds = clf(batch_x)
        loss = loss_fn(preds, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}: loss = {total_loss:.4f}")

clf.eval()
correct = 0
total = 0
with torch.no_grad():
    for batch_x, batch_y in test_loader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        preds = clf(batch_x).squeeze()
        predicted = (preds > 0.5).long()
        correct += (predicted == batch_y.long()).sum().item()
        total += batch_y.size(0)

print(f"Test accuracy: {correct / total:.2%}")