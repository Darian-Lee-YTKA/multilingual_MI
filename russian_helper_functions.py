
import pandas as pd
import sys
from pathlib import Path
import torch as t
from torch import Tensor
import numpy as np
import einops
from tqdm.notebook import tqdm
import plotly.express as px
import re
import itertools
from jaxtyping import Float, Int, Bool
from typing import Literal, Callable
from functools import partial
from IPython.display import display, HTML
from rich.table import Table, Column
from rich import print as rprint
import circuitsvis as cv
from transformer_lens.hook_points import HookPoint
from transformer_lens import utils, HookedTransformer, ActivationCache
from transformer_lens.components import Embed, Unembed, LayerNorm, MLP
import pandas as pd
import os


import torch
import torch.nn.functional as F
from transformer_lens import HookedTransformer
from transformers import AutoTokenizer
import utils  # —Ç–≤–æ–π –º–æ–¥—É–ª—å

import torch
import torch.nn.functional as F
from transformer_lens import HookedTransformer
from transformers import AutoTokenizer

import torch
import torch.nn.functional as F
from transformer_lens import HookedTransformer
from transformers import AutoTokenizer

# üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ‚Äî –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ cuda:3, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")

print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# üß† –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Mistral –Ω–∞ device
model = HookedTransformer.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    center_unembed=True,
    center_writing_weights=True,
    fold_ln=True,
    refactor_factored_attn_matrices=False,
    device=device,  # –≤–∞–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
)
model.to(device)  # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ device –Ω–µ—Ç)
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

# üîç –§—É–Ω–∫—Ü–∏—è –ª–æ–≥–∏—Ç-–ª–∏–Ω–∑—ã —Å —è–≤–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–æ–º –≤—Å–µ—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ –Ω–∞ device
def run_logit_lens(text: str):
    tokens = model.to_tokens(text, prepend_bos=False).to(device)
    token_strs = model.to_str_tokens(tokens)

    logits, cache = model.run_with_cache(tokens)

    print(f"\nüîç Logit lens –¥–ª—è —Å—Ç—Ä–æ–∫–∏: {text!r}")
    print(f"üìé –¢–æ–∫–µ–Ω—ã: {token_strs}\n")

    for layer in range(model.cfg.n_layers):
        resid = cache["resid_post", layer][0, -1]
        if resid.device != device:
            resid = resid.to(device)

        logits_lens = model.unembed(resid)
        if logits_lens.device != device:
            logits_lens = logits_lens.to(device)

        probs = F.softmax(logits_lens, dim=-1)
        topk = torch.topk(probs, k=5)

        print(f"--- –°–ª–æ–π {layer} ---")
        for i in range(5):
            token = model.to_string(topk.indices[i])
            prob = topk.values[i].item()
            print(f"{i+1}. {token!r:<10} ‚Üí {prob:.4f}")

# üéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ (–±–µ–∑ utils), —Ç–∞–∫–∂–µ –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ device
def test_prompt(prompt: str, target: str):
    print(f"\nüéØ –ü—Ä–æ–≤–µ—Ä–∫–∞: prompt = {prompt!r}, target = {target!r}")
    tokens = model.to_tokens(prompt + target, prepend_bos=True).to(device)

    logits = model(tokens)
    target_token = tokens[0, -1]

    last_logits = logits[0, -2]
    if last_logits.device != device:
        last_logits = last_logits.to(device)

    probs = F.softmax(last_logits, dim=-1)
    target_prob = probs[target_token].item()

    print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∞ {target!r}: {target_prob:.4f}")

# –ó–∞–ø—É—Å–∫–∞–µ–º –ª–æ–≥–∏—Ç-–ª–∏–Ω–∑—É
run_logit_lens("Ich liebe")
run_logit_lens("Ich gab")

# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å –¥–≤—É–º—è –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤
for prompt in ["Ich liebe ", "Ich gab "]:
    for answer in ["den", "dem"]:
        test_prompt(prompt, answer)
