import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch.nn.functional as F
import matplotlib
import matplotlib.pyplot as plt
# –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏ –º–æ–¥–µ–ª—å
device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
model_name = "sberbank-ai/rugpt3large_based_on_gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(device)
model.eval()

# –î–∞–Ω–Ω—ã–µ
print("tokeizer")
print(tokenizer.tokenize("—Å–≤–æ–µ–π"))
print(tokenizer.tokenize("—Ç–µ–±—è"))
print(tokenizer.tokenize("–µ–≥–æ"))
print(tokenizer.tokenize("—Å–≤–æ–∏–º"))
true = ["–æ–Ω–∞ –¥–æ—Ä–æ–∂–∏—Ç", "—è –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞—Å—å —Å"]
true_case = ["—Å–≤–æ–µ–π", "—Å–≤–æ–∏–º", "–Ω–∏–º", "—Ç–µ–º"]
corrupted_acc = ["—Ç–µ–±—è", "–µ–≥–æ", "—Ç–æ–≥–æ", "–≤–æ–ø—Ä–æ—Å–∞"]
corrupted = ["–æ–Ω–∞ –ª—é–±–∏—Ç", "—è –Ω–∞—á–∞–ª–∞ —Å"]

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
clean_attn_output = []
corrupt_attn_output = []

attn_weights_storage = []

def get_attention_weights(prompt, storage):
    storage.clear()
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs, output_attentions=True)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    last_hidden = outputs.hidden_states[-1].detach().cpu()  # [batch, seq_len, embed_dim]
    storage.append(last_hidden)
    
    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
    attentions = outputs.attentions  # —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –ø–æ —Å–ª–æ—è–º
    last_layer_attn = attentions[-1].detach().cpu()  # [batch, heads, seq_len, seq_len]
    storage.append(last_layer_attn)
    
    return inputs


def print_topk_logits(hidden, k=5):
    logits = model.lm_head(hidden[0, -1])
    topk = torch.topk(logits, k)
    tokens = tokenizer.batch_decode(topk.indices)
    print("üîù Top-5 predictions:")
    for token, score in zip(tokens, topk.values):
        print(f"   {token.strip():>10} ({score.item():.2f})")

# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–æ–≤ attention —Å–ª–æ—è
def get_attention_output(prompt, storage):
    storage.clear()
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    def hook(module, input, output):
        storage.append(output[0].detach())

    handle = model.transformer.h[-1].attn.register_forward_hook(hook)
    with torch.no_grad():
        _ = model(**inputs)
    handle.remove()
    return inputs




def get_logits_diff_by_category(hidden, positive_words, negative_words, top_k=1000):
    logits = model.lm_head(hidden[0, -1])
    topk = torch.topk(logits, top_k)
    tokens = tokenizer.batch_decode(topk.indices)

    top_pos = float('-inf')
    top_neg = float('-inf')
    best_pos = None
    best_neg = None

    for logit, token in zip(topk.values, tokens):
        stripped_token = token.strip()

        if stripped_token in positive_words and logit > top_pos:
            top_pos = logit.item()
            best_pos = stripped_token
        elif stripped_token in negative_words and logit > top_neg:
            top_neg = logit.item()
            best_neg = stripped_token

    if best_pos is None:
        print(f"‚ö†Ô∏è –ù–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏–∑ {positive_words} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-{top_k}")
    if best_neg is None:
        print(f"‚ö†Ô∏è –ù–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –∏–∑ {negative_words} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-{top_k}")

    print(f"‚úÖ Top positive: {best_pos} ({top_pos:.2f})")
    print(f"‚ùå Top negative: {best_neg} ({top_neg:.2f})")

    return top_pos - top_neg



# –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–∏—Ç–æ–≤ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–∫—Ä—ã—Ç–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é
def get_logits_from_hidden(hidden, target_words, top_k=1000):
    logits = model.lm_head(hidden[0, -1])  # logits –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
    topk = torch.topk(logits, top_k)  # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø top_k –ª–æ–≥–∏—Ç–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–æ–≤
    top_probs = torch.softmax(topk.values, dim=-1)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ª–æ–≥–∏—Ç—ã –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    top_tokens = tokenizer.batch_decode(topk.indices)  # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ —Ç–æ–∫–µ–Ω—ã

    result = []
    for word in target_words:
        found = False
        word = " " + word
        for prob, token in zip(top_probs, top_tokens):
            if token == word or token == word[1:]:
                result.append(prob.item())
                found = True
                break
            if word == " —Ç–µ–±—è":
                if token == "–µ–≥–æ" or token == " –µ–≥–æ": # –µ–≥–æ might be more likely incorrect than —Ç–µ–±—è
                    result.append(prob.item())
                    found = True
                    break
        if not found:
            print(f"‚ö†Ô∏è –°–ª–æ–≤–æ '{word}' –∏ —Å–ª–æ–≤–æ '{word[1:]}' –ù–ï –Ω–∞–π–¥–µ–Ω–æ —Å—Ä–µ–¥–∏ —Ç–æ–ø-{top_k} —Ç–æ–∫–µ–Ω–æ–≤")
            result.append(0.0)
    return result

# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
def get_last_hidden(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
    return outputs.hidden_states[-1]

# –ü–æ–¥–º–µ–Ω–∞ –æ–¥–Ω–æ–π attention –≥–æ–ª–æ–≤—ã
def patch_single_head(orig, corrupt, head_idx, head_size):
    patched = orig.clone()
    start = head_idx * head_size
    end = (head_idx + 1) * head_size
    patched[0, -1, start:end] = corrupt[0, -1, start:end]
    return patched

# –ó–∞–ø—É—Å–∫ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–º–µ–Ω—ë–Ω–Ω—ã–º attention –≤—ã—Ö–æ–¥–æ–º
def run_with_patched_attn(patched_attn, prompt, target_words):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    def hook(module, input, output):
        return (patched_attn, ) + output[1:]


    handle = model.transformer.h[-1].attn.register_forward_hook(hook)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        hidden = outputs.hidden_states[-1]
        logits = get_logits_from_hidden(hidden, target_words)
        print_topk_logits(hidden)  # ‚Üê –∑–¥–µ—Å—å
    handle.remove()
    
    return logits


def plot_attention(attn_weights, head_idx, tokens, title=None):
    # –ï—Å–ª–∏ attn_weights ‚Äî –∫–æ—Ä—Ç–µ–∂, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
    if isinstance(attn_weights, tuple):
        attn_weights = attn_weights[0]
    
    # attn_weights shape: [batch_size, num_heads, seq_len, seq_len]
    print(attn_weights.shape)
    attn = attn_weights[0, head_idx].cpu().numpy()  # shape: [seq_len, seq_len]

    plt.figure(figsize=(8, 6))
    plt.imshow(attn, cmap="viridis")
    plt.colorbar()
    plt.xticks(ticks=range(len(tokens)), labels=tokens, rotation=90)
    plt.yticks(ticks=range(len(tokens)), labels=tokens)
    if title:
        plt.title(title)
    plt.tight_layout()
    plt.show()





# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
n_heads = model.config.n_head
head_size = model.config.n_embd // n_heads




for i, (clean_prompt, corrupt_prompt, correct, wrong) in enumerate(zip(true, corrupted, true_case, corrupted_acc)):
    print(f"\nüß™ –ü—Ä–∏–º–µ—Ä {i+1}: {clean_prompt}")

    # –ü–æ–ª—É—á–∞–µ–º attention-–≤—ã—Ö–æ–¥—ã
    clean_inputs = get_attention_output(clean_prompt, clean_attn_output)
    corrupt_inputs = get_attention_output(corrupt_prompt, corrupt_attn_output)
    clean_attn = clean_attn_output[0]  # [1, seq_len, emb_dim]
    corrupt_attn = corrupt_attn_output[0]

    # –õ–æ–≥–∏—Ç—ã –±–µ–∑ –ø–∞—Ç—á–∏–Ω–≥–∞
    with torch.no_grad():
        clean_hidden = get_last_hidden(clean_prompt)
        clean_logits = get_logits_from_hidden(clean_hidden, [correct, wrong])
        clean_diff = get_logits_diff_by_category(clean_hidden, true_case, corrupted_acc)
        print(f"‚úÖ –ë–µ–∑ –ø–∞—Ç—á–∞: {correct} = {clean_logits[0]:.4f}, {wrong} = {clean_logits[1]:.4f} ‚Üí diff = {clean_diff:.4f}")
        print_topk_logits(clean_hidden)  # üîç –≤–æ—Ç —Ç—É—Ç


    # –ü–µ—Ä–µ–±–æ—Ä –≥–æ–ª–æ–≤
    head_deltas = []


    for head in range(n_heads):
        patched = patch_single_head(clean_attn, corrupt_attn, head, head_size)
        patched_logits = run_with_patched_attn(patched, clean_prompt, [correct, wrong])
        patched_diff = get_logits_diff_by_category(patched, true_case, corrupted_acc)
        delta = patched_diff - clean_diff

        head_deltas.append((head, delta))
        print(f"üîÅ Head {head:2d}: Patched diff = {patched_diff:.4f}, Œî = {delta:+.4f}")
        print("difference from clean diff: ", patched_diff - clean_diff)

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –≥–æ–ª–æ–≤ –ø–æ –≤–ª–∏—è–Ω–∏—é
    
    head_deltas.sort(key=lambda x: abs(x[1]), reverse=True)
    print("\nüìä üß† printing for prompt: ", clean_prompt, corrupt_prompt)
    for rank, (head, delta) in enumerate(head_deltas, 1):
        print(f"   {rank}. Head {head:2d}: Œî = {delta:+.4f}")
    
    get_attention_weights(clean_prompt, attn_weights_storage)
    clean_attn_weights = attn_weights_storage[0]
    tokens = clean_prompt.split(" ")
    for i in range(4):
        head, delta = head_deltas[i]
        print(f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≥–æ–ª–æ–≤—ã {head} —Å Œî={delta:.4f}")

        plot_attention(clean_attn_weights, head, tokens, title=f"Clean prompt attention")
